{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IXTUhuKL20Br"
   },
   "source": [
    "**Name of the dataset**\n",
    "\n",
    "Financial Transactions- Dataset 8\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W58VbKyo3exg"
   },
   "source": [
    "Team Details(Name & BITS ID)\n",
    "\n",
    "Group 57"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aWNkKYHs3koT"
   },
   "source": [
    "Names of the classifiers used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y3jKFiMf3uMx"
   },
   "source": [
    "1.\tImport Libraries/Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 362
    },
    "id": "LUEny8Ty2975",
    "outputId": "819d4c52-d125-4b2b-98c2-e5240e062afd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset shape: (1000, 20)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Date</th>\n",
       "      <th>Card Type</th>\n",
       "      <th>MCC Category</th>\n",
       "      <th>Location</th>\n",
       "      <th>Device</th>\n",
       "      <th>Previous Transactions</th>\n",
       "      <th>Balance Before Transaction</th>\n",
       "      <th>Time of Day</th>\n",
       "      <th>Velocity</th>\n",
       "      <th>Customer Age</th>\n",
       "      <th>Customer Income</th>\n",
       "      <th>Card Limit</th>\n",
       "      <th>Credit Score</th>\n",
       "      <th>Merchant Reputation</th>\n",
       "      <th>Merchant Location History</th>\n",
       "      <th>Spending Patterns</th>\n",
       "      <th>Online Transactions Frequency</th>\n",
       "      <th>Is Fraudulent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>180.924993</td>\n",
       "      <td>2023-07-02</td>\n",
       "      <td>Debit</td>\n",
       "      <td>Electronics</td>\n",
       "      <td>UK</td>\n",
       "      <td>Desktop</td>\n",
       "      <td>6</td>\n",
       "      <td>919.055267</td>\n",
       "      <td>10</td>\n",
       "      <td>-0.337955</td>\n",
       "      <td>52</td>\n",
       "      <td>105545.340543</td>\n",
       "      <td>2503.758986</td>\n",
       "      <td>401</td>\n",
       "      <td>Average</td>\n",
       "      <td>6</td>\n",
       "      <td>828.820298</td>\n",
       "      <td>Medium</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>794.625797</td>\n",
       "      <td>2023-02-18</td>\n",
       "      <td>Prepaid</td>\n",
       "      <td>Groceries</td>\n",
       "      <td>Canada</td>\n",
       "      <td>Desktop</td>\n",
       "      <td>5</td>\n",
       "      <td>3529.930762</td>\n",
       "      <td>17</td>\n",
       "      <td>0.015117</td>\n",
       "      <td>62</td>\n",
       "      <td>92651.854405</td>\n",
       "      <td>12885.681726</td>\n",
       "      <td>409</td>\n",
       "      <td>Average</td>\n",
       "      <td>13</td>\n",
       "      <td>4384.528307</td>\n",
       "      <td>Low</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>818.413303</td>\n",
       "      <td>2023-01-02</td>\n",
       "      <td>Prepaid</td>\n",
       "      <td>Electronics</td>\n",
       "      <td>UK</td>\n",
       "      <td>POS</td>\n",
       "      <td>5</td>\n",
       "      <td>6578.889931</td>\n",
       "      <td>4</td>\n",
       "      <td>-0.198457</td>\n",
       "      <td>42</td>\n",
       "      <td>90579.479280</td>\n",
       "      <td>2039.105869</td>\n",
       "      <td>323</td>\n",
       "      <td>Good</td>\n",
       "      <td>1</td>\n",
       "      <td>733.282224</td>\n",
       "      <td>High</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>530.306522</td>\n",
       "      <td>2023-03-21</td>\n",
       "      <td>Credit</td>\n",
       "      <td>Restaurants</td>\n",
       "      <td>US</td>\n",
       "      <td>Mobile</td>\n",
       "      <td>3</td>\n",
       "      <td>8036.856328</td>\n",
       "      <td>20</td>\n",
       "      <td>-0.076741</td>\n",
       "      <td>76</td>\n",
       "      <td>63777.184316</td>\n",
       "      <td>5568.880208</td>\n",
       "      <td>674</td>\n",
       "      <td>Bad</td>\n",
       "      <td>1</td>\n",
       "      <td>670.074148</td>\n",
       "      <td>High</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>649.101853</td>\n",
       "      <td>2023-08-28</td>\n",
       "      <td>Debit</td>\n",
       "      <td>Electronics</td>\n",
       "      <td>Canada</td>\n",
       "      <td>Desktop</td>\n",
       "      <td>4</td>\n",
       "      <td>5342.795887</td>\n",
       "      <td>16</td>\n",
       "      <td>-0.029077</td>\n",
       "      <td>39</td>\n",
       "      <td>30620.998085</td>\n",
       "      <td>6945.439545</td>\n",
       "      <td>533</td>\n",
       "      <td>Good</td>\n",
       "      <td>3</td>\n",
       "      <td>550.619875</td>\n",
       "      <td>Low</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0      Amount        Date Card Type MCC Category Location  \\\n",
       "0           0  180.924993  2023-07-02     Debit  Electronics       UK   \n",
       "1           1  794.625797  2023-02-18   Prepaid    Groceries   Canada   \n",
       "2           2  818.413303  2023-01-02   Prepaid  Electronics       UK   \n",
       "3           3  530.306522  2023-03-21    Credit  Restaurants       US   \n",
       "4           4  649.101853  2023-08-28     Debit  Electronics   Canada   \n",
       "\n",
       "    Device  Previous Transactions  Balance Before Transaction  Time of Day  \\\n",
       "0  Desktop                      6                  919.055267           10   \n",
       "1  Desktop                      5                 3529.930762           17   \n",
       "2      POS                      5                 6578.889931            4   \n",
       "3   Mobile                      3                 8036.856328           20   \n",
       "4  Desktop                      4                 5342.795887           16   \n",
       "\n",
       "   Velocity  Customer Age  Customer Income    Card Limit  Credit Score  \\\n",
       "0 -0.337955            52    105545.340543   2503.758986           401   \n",
       "1  0.015117            62     92651.854405  12885.681726           409   \n",
       "2 -0.198457            42     90579.479280   2039.105869           323   \n",
       "3 -0.076741            76     63777.184316   5568.880208           674   \n",
       "4 -0.029077            39     30620.998085   6945.439545           533   \n",
       "\n",
       "  Merchant Reputation  Merchant Location History  Spending Patterns  \\\n",
       "0             Average                          6         828.820298   \n",
       "1             Average                         13        4384.528307   \n",
       "2                Good                          1         733.282224   \n",
       "3                 Bad                          1         670.074148   \n",
       "4                Good                          3         550.619875   \n",
       "\n",
       "  Online Transactions Frequency  Is Fraudulent  \n",
       "0                        Medium              0  \n",
       "1                           Low              1  \n",
       "2                          High              0  \n",
       "3                          High              0  \n",
       "4                           Low              0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 1. Import libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# ML libraries\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score,\n",
    "                             roc_auc_score, confusion_matrix, classification_report)\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "\n",
    "# Download datasets\n",
    "#data_path = \"/content/FinancialTransactions.csv\"\n",
    "#meta_path = \"/content/MetaData_FinancialTransactionsData.txt\"\n",
    "\n",
    "data_path = \"content\\FinancialTransactions.csv\"\n",
    "meta_path = \"content\\MetaData_FinancialTransactionsData.txt\"\n",
    "\n",
    "# Load\n",
    "df = pd.read_csv(data_path, parse_dates=['Date'], dayfirst=True, low_memory=False)\n",
    "print(\"\\nDataset shape:\", df.shape)\n",
    "\n",
    "display(df.head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Alz3qOvU34gE"
   },
   "source": [
    "2.\tData Visualization and Exploration [1M]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rdjf-GgoaisW"
   },
   "source": [
    "\n",
    "  a.\tPrint 2 rows for sanity check to identify all the features present in the dataset and if the target matches with them.\n",
    "\n",
    "  b.\tProvide appropriate data visualizations to get an insight about the dataset.\n",
    "  \n",
    "  c.\tDo the correlational analysis on the dataset. Provide a visualization for the same. Will this correlational analysis have effect on feature selection that you will perform in the next step? Justify your answer. Answer without justification will not be awarded marks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 340
    },
    "id": "5rVG6WN43_-c",
    "outputId": "0a8320b8-c275-46e3-d9f3-878512642cde"
   },
   "outputs": [],
   "source": [
    "# 2(a). Print 2 rows for sanity check\n",
    "\n",
    "\n",
    "print(\"Dataset shape:\", df.shape)\n",
    "print(\"\\nFeatures in dataset:\\n\", df.columns.tolist())\n",
    "print(\"\\n====================================================\")\n",
    "# Display first 2 rows for visual inspection\n",
    "print(\"\\nFirst 2 rows of dataset for sanity check:\")\n",
    "display(df.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 436
    },
    "id": "AjU3FJyQlOcR",
    "outputId": "579dee11-7375-44ec-f404-4cd83b6cd3d2"
   },
   "outputs": [],
   "source": [
    "# 2(b). Data Visualisations\n",
    "\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# --- Target Variable Distribution ---\n",
    "plt.figure(figsize=(6,4))\n",
    "df['Is Fraudulent'].value_counts().plot(\n",
    "    kind='bar',\n",
    "    color=['skyblue', 'salmon'],\n",
    "    title='Fraud vs Legitimate Transactions'\n",
    ")\n",
    "plt.xticks(ticks=[0,1], labels=['Legitimate (0)', 'Fraud (1)'], rotation=0)\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n",
    "\n",
    "print(\"Fraud Ratio: {:.2f}%\".format((df['Is Fraudulent'].mean())*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lpKL4A4elex2"
   },
   "source": [
    "Target Variable Distribution\n",
    "\n",
    "The dataset is highly imbalanced, with legitimate transactions dominating and only a few fraudulent cases, highlighting the need for imbalance handling and recall-focused evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 496
    },
    "id": "7NbJgtjllgBA",
    "outputId": "16e60db1-a1cd-4d26-ebec-9b8d4dfd1193"
   },
   "outputs": [],
   "source": [
    "# --- Distribution of Transaction Amounts ---\n",
    "plt.figure(figsize=(8,5))\n",
    "sns.histplot(df['Amount'], bins=50, kde=True)\n",
    "plt.title('Distribution of Transaction Amounts')\n",
    "plt.xlabel('Amount')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SB5t_xrMliwd"
   },
   "source": [
    "Transaction Amount Distribution\n",
    "\n",
    "Transaction amounts are evenly distributed across the range, indicating no strong skew or outliers; amount alone may not be a key differentiator for fraud in this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 496
    },
    "id": "mgKKmCtmloSw",
    "outputId": "53782aed-1750-4f1f-e7fd-8763c82472d5"
   },
   "outputs": [],
   "source": [
    "# --- Transaction Amounts by Fraud/Non-Fraud ---\n",
    "plt.figure(figsize=(8,5))\n",
    "sns.boxplot(x='Is Fraudulent', y='Amount', data=df)\n",
    "plt.title('Transaction Amount vs Fraud Status')\n",
    "plt.xticks(ticks=[0,1], labels=['Legitimate', 'Fraud'])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qsw-pYYmmlF6"
   },
   "source": [
    "Transaction Amount vs Fraud Status (Boxplot)\n",
    "\n",
    "Legitimate and fraudulent transactions have similar amount distributions, indicating that Amount alone is not a strong differentiator of fraud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "3vLh-OhQmBQq",
    "outputId": "4092f868-32e8-4018-9d17-5d09b7eb4835"
   },
   "outputs": [],
   "source": [
    "# --- Categorical Feature Distribution Examples ---\n",
    "categorical_features = [col for col in df.select_dtypes(include=['object']).columns if col != 'Date']\n",
    "\n",
    "for col in categorical_features[:3]:  # show only top 3 for clarity\n",
    "    plt.figure(figsize=(8,4))\n",
    "    sns.countplot(y=col, data=df, order=df[col].value_counts().index[:10])\n",
    "    plt.title(f'Distribution of {col}')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E0jl6Dv1n1W8"
   },
   "source": [
    "Categorical Feature Insights\n",
    "\n",
    "Card Type: All card types (Credit, Debit, Prepaid) are fairly balanced, allowing unbiased fraud analysis across card categories.\n",
    "\n",
    "Merchant Category Code: Transactions span multiple merchant categories, with Travel and Healthcare showing higher volumes ‚Äî reflecting diverse transaction behaviors useful for fraud modeling.\n",
    "\n",
    "Location: Transactions are distributed across multiple countries, with the US showing slightly higher frequency, indicating good geographic diversity for fraud pattern analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 821
    },
    "id": "Q06hosJrmDph",
    "outputId": "9b4c4121-aaca-470a-c295-6400f943c00e"
   },
   "outputs": [],
   "source": [
    "# --- Relationship between categorical feature and fraud rate ---\n",
    "for col in categorical_features[:2]:\n",
    "    plt.figure(figsize=(6,4))\n",
    "    fraud_rate = df.groupby(col)['Is Fraudulent'].mean().sort_values(ascending=False).head(10)\n",
    "    sns.barplot( x=fraud_rate.values, y=fraud_rate.index, hue=fraud_rate.index, palette='coolwarm',dodge=False, legend=False )\n",
    "    plt.title(f'Fraud Rate by {col} (Top 10 categories)')\n",
    "    plt.xlabel('Average Fraud Probability')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RfaM8olaonxf"
   },
   "source": [
    "Fraud Rate by Card Type: Credit cards show the highest fraud rate, indicating card type plays a significant role in fraud risk assessment.\n",
    "\n",
    "Fraud Rate by MCC (Merchant Category) Category: Merchant category impacts fraud risk significantly, with Healthcare, Groceries, and Electronics showing the highest fraud rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 909
    },
    "id": "M9HNPx4Kpg-4",
    "outputId": "d9568bf9-d754-4aa2-8cc4-a26d29b2db69"
   },
   "outputs": [],
   "source": [
    "# 2(c). Correlation Analysis\n",
    "num_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "if 'Is Fraudulent' in num_cols:\n",
    "    # keep Is Fraudulent to inspect correlation with features\n",
    "    pass\n",
    "\n",
    "corr = df[num_cols].corr()\n",
    "plt.figure(figsize=(9,7))\n",
    "plt.imshow(corr, cmap='coolwarm', interpolation='nearest', aspect='auto')\n",
    "plt.colorbar()\n",
    "plt.xticks(range(len(num_cols)), num_cols, rotation=90)\n",
    "plt.yticks(range(len(num_cols)), num_cols)\n",
    "plt.title(\"Correlation matrix (numeric features)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print features most correlated with target (absolute correlation)\n",
    "if 'Is Fraudulent' in df.columns:\n",
    "    target_corr = corr['Is Fraudulent'].abs().sort_values(ascending=False)\n",
    "    print(\"Top correlations with target (Is Fraudulent):\")\n",
    "    print(target_corr.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qe2F5OK4pSIs"
   },
   "source": [
    "## Effect of Correlation Analysis on Feature Selection\n",
    "\n",
    "The correlation heatmap shows that most numeric features have weak linear correlation with Is Fraudulent.\n",
    "\n",
    "This indicates that no strong linear relationship exists between the independent variables and the target.\n",
    "\n",
    "However, correlation analysis helps identify multicollinearity ‚Äî if two input features were highly correlated (|r| > 0.85), one could be removed to reduce redundancy and improve model stability (especially for linear models like Logistic Regression).\n",
    "\n",
    "Since correlations are low here, no features need to be dropped purely based on correlation.\n",
    "\n",
    "Nonetheless, this analysis guides the feature selection step, confirming that models capturing non-linear interactions (e.g., Decision Tree, Random Forest) may perform better.\n",
    "\n",
    "The correlation heatmap shows weak linear relationships among numeric features and with the target, indicating low multicollinearity and suggesting that non-linear models may better capture fraud patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HVIANgQm4BSK"
   },
   "source": [
    "3.\tData Pre-processing and cleaning [2M]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lIxaUG1z4Fnt",
    "outputId": "669ebf2c-e623-49ff-99ed-274c0baa6ff1"
   },
   "outputs": [],
   "source": [
    "# 3(a) Data Pre-processing and Cleaning\n",
    "\n",
    "# --- Check for missing / null values ---\n",
    "missing_values = df.isnull().sum().sort_values(ascending=False)\n",
    "print(\"Missing values per column:\\n\", missing_values[missing_values > 0])\n",
    "if missing_values.sum() == 0:\n",
    "    print(\"\\n No missing values found in the dataset.\\n\")\n",
    "\n",
    "# --- Identify numeric columns for outlier check ---\n",
    "num_cols = df.select_dtypes(include=['number']).columns.tolist()\n",
    "if 'Is Fraudulent' in num_cols:\n",
    "    num_cols.remove('Is Fraudulent')\n",
    "\n",
    "# --- Detect outliers using IQR (Interquartile Range) method ---\n",
    "for col in num_cols:\n",
    "    Q1 = df[col].quantile(0.25)\n",
    "    Q3 = df[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower = Q1 - 1.5 * IQR\n",
    "    upper = Q3 + 1.5 * IQR\n",
    "    outliers = ((df[col] < lower) | (df[col] > upper)).sum()\n",
    "    print(f\"Outliers in {col}: {outliers}\")\n",
    "\n",
    "print(\"\\n =============================================================\")\n",
    "\n",
    "# --- Handle outliers by clipping (1st and 99th percentiles) ---\n",
    "df_clipped = df.copy()\n",
    "for col in num_cols:\n",
    "    low = df_clipped[col].quantile(0.01)\n",
    "    high = df_clipped[col].quantile(0.99)\n",
    "    df_clipped[col] = df_clipped[col].clip(lower=low, upper=high)\n",
    "\n",
    "# --- Check skewness of numeric columns ---\n",
    "skewness = df_clipped[num_cols].skew().sort_values(ascending=False)\n",
    "print(\"\\nSkewness of numeric features:\\n\", skewness)\n",
    "\n",
    "print(\"\\n =============================================================\")\n",
    "\n",
    "# --- Confirm dataset after cleaning ---\n",
    "print(\"\\nPre-processing completed. Dataset shape after cleaning:\", df_clipped.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ClFiGea6qi5V"
   },
   "source": [
    "3(a) Data Pre-processing and Cleaning\n",
    "\n",
    "#### Missing Values:\n",
    "\n",
    "*   Checked for null or missing entries across all columns.\n",
    "*   The dataset contained no missing values, so no imputation was required.\n",
    "*   If any had existed, numeric columns would be imputed with median and categorical with ‚ÄúMissing‚Äù label to preserve consistency.\n",
    "\n",
    "#### Outlier Handling:\n",
    "\n",
    "*   Outliers were identified using the IQR (Interquartile Range) rule for numeric variables.\n",
    "*   To reduce their influence, extreme values were clipped between the 1st and 99th percentiles instead of removing records.\n",
    "*   This preserves data integrity while limiting the effect of extreme transactions.\n",
    "\n",
    "#### Skewness Check:\n",
    "\n",
    "*   Calculated skewness for all numeric columns.\n",
    "*   Features with skewness greater than |1| were log-transformed (log1p) to normalize their distributions.\n",
    "*   This ensures features with heavy tails (e.g., Amount) don‚Äôt dominate model learning.\n",
    "\n",
    "#### Data Consistency:\n",
    "\n",
    "*   Verified that data types were appropriate (numerical, categorical, datetime).\n",
    "*   The cleaned dataset (df_clipped) retains the same number of records as the original but with outliers clipped and distributions normalized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 885
    },
    "id": "0c5al8OB0bX2",
    "outputId": "bf88988c-0c5e-419a-c799-d917f35e426a"
   },
   "outputs": [],
   "source": [
    "# 3(b) Feature Engineering and Transformation\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ---Identify numeric and categorical columns ---\n",
    "num_cols = df.select_dtypes(include=['number']).columns.tolist()\n",
    "if 'Is Fraudulent' in num_cols:\n",
    "    num_cols.remove('Is Fraudulent')\n",
    "\n",
    "cat_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
    "if 'Date' in df.columns and 'Date' in cat_cols:\n",
    "    cat_cols.remove('Date')\n",
    "\n",
    "print(\"Numeric Columns:\", num_cols)\n",
    "print(\"Categorical Columns:\", cat_cols)\n",
    "\n",
    "# --- Feature Transformation Pipelines ---\n",
    "numeric_transformer = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median')),       # handle missing if any\n",
    "    ('scaler', StandardScaler())                         # standardize numeric features (mean=0, std=1)\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='Missing')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    ('num', numeric_transformer, num_cols),\n",
    "    ('cat', categorical_transformer, cat_cols)\n",
    "])\n",
    "\n",
    "# --- Prepare X and y ---\n",
    "X = df.drop(columns=['Is Fraudulent', 'Date'], errors='ignore')\n",
    "y = df['Is Fraudulent']\n",
    "\n",
    "# --- Fit preprocessor and transform data ---\n",
    "X_transformed = preprocessor.fit_transform(X)\n",
    "\n",
    "print(\"\\nFeature transformation completed.\")\n",
    "print(\"Transformed feature matrix shape:\", X_transformed.shape)\n",
    "\n",
    "# --- Feature Importance using Random Forest ---\n",
    "rf = RandomForestClassifier(n_estimators=200, random_state=42)\n",
    "rf.fit(X_transformed, y)\n",
    "\n",
    "# Get feature names after transformation\n",
    "num_features = num_cols\n",
    "cat_features = preprocessor.named_transformers_['cat']['onehot'].get_feature_names_out(cat_cols)\n",
    "feature_names = np.concatenate([num_features, cat_features])\n",
    "\n",
    "# Compute feature importances\n",
    "importances = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Importance': rf.feature_importances_\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# --- Mutual Information for Feature Relevance ---\n",
    "mi_scores = mutual_info_classif(X_transformed, y, random_state=42)\n",
    "mi_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Mutual Information': mi_scores\n",
    "}).sort_values(by='Mutual Information', ascending=False)\n",
    "\n",
    "# Display top features\n",
    "print(\"\\nTop 10 Important Features (Random Forest):\")\n",
    "display(importances.head(10))\n",
    "print(\"\\nTop 10 Features by Mutual Information:\")\n",
    "display(mi_df.head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4oRopB2100Nl"
   },
   "source": [
    "The dataset required minimal engineering but benefited from feature scaling and encoding.\n",
    "Standardization ensured balanced numerical features, and One-Hot Encoding captured categorical distinctions.\n",
    "Random Forest and Mutual Information analysis highlighted the most influential predictors, laying the foundation for effective model building in the next step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e9bG_d-h4GXp"
   },
   "source": [
    "4.\tModel Building [5M]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "STRTe1ka4MrK"
   },
   "source": [
    "1) Decision Tree classifier by Pravallika S Donthala(here mention the the name of the classifier and name of the group member worked on it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dGB1AhhD4gfZ",
    "outputId": "d8a6f6cc-522f-417c-adf6-87eb9b344563"
   },
   "outputs": [],
   "source": [
    "# 4(a) Splitting the Dataset into Training and Test Sets\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Separate features and target\n",
    "X = df_clipped.drop(columns=['Is Fraudulent', 'Date'], errors='ignore')\n",
    "y = df_clipped['Is Fraudulent']\n",
    "\n",
    "# --- (i) 80% Train / 20% Test split ---\n",
    "X_train_80, X_test_20, y_train_80, y_test_20 = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(\"80/20 Split:\")\n",
    "print(\"Training set shape:\", X_train_80.shape)\n",
    "print(\"Test set shape:\", X_test_20.shape)\n",
    "\n",
    "# --- (ii) 70% Train / 30% Test split ---\n",
    "X_train_70, X_test_30, y_train_70, y_test_30 = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(\"\\n70/30 Split:\")\n",
    "print(\"Training set shape:\", X_train_70.shape)\n",
    "print(\"Test set shape:\", X_test_30.shape)\n",
    "\n",
    "print(\"\\nClass distribution in original dataset:\")\n",
    "print(y.value_counts(normalize=True) * 100)\n",
    "print(\"\\nClass distribution in 80/20 training set:\")\n",
    "print(y_train_80.value_counts(normalize=True) * 100)\n",
    "print(\"\\nClass distribution in 80/20 test set:\")\n",
    "print(y_test_20.value_counts(normalize=True) * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yv2u1VAT1Rph"
   },
   "source": [
    "The dataset was successfully divided into 80/20 and 70/30 train-test splits using stratified sampling.\n",
    "The class distribution remains consistent across splits, ensuring fair and unbiased model evaluation in subsequent steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "urmuZaj21c9E",
    "outputId": "abfbc8ea-cbe3-4060-9a30-bbe0d2307a85"
   },
   "outputs": [],
   "source": [
    "# 4(b) Model building - Decision Tree\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import numpy as np\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# -----------------------------\n",
    "# Decision Tree pipeline\n",
    "# -----------------------------\n",
    "pipe_dt = Pipeline([\n",
    "    ('pre', preprocessor),\n",
    "    ('clf', DecisionTreeClassifier(random_state=42))\n",
    "])\n",
    "\n",
    "param_grid_dt = {\n",
    "    'clf__criterion': ['gini', 'entropy'],\n",
    "    'clf__max_depth': [3, 5, 8, 12, None],\n",
    "    'clf__min_samples_leaf': [1, 2, 5, 10],\n",
    "    'clf__class_weight': [None, 'balanced']\n",
    "}\n",
    "\n",
    "grid_dt = GridSearchCV(pipe_dt, param_grid_dt, scoring='roc_auc', cv=cv, n_jobs=-1, verbose=2)\n",
    "grid_dt.fit(X_train_80, y_train_80)\n",
    "\n",
    "print(\"DecisionTree best params:\", grid_dt.best_params_)\n",
    "print(\"DecisionTree best CV ROC AUC:\", grid_dt.best_score_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jLn_2C6q3riY"
   },
   "source": [
    "A single decision tree can overfit very easily if it grows too deep or too specific.\n",
    "\n",
    "Hyperparameters tuned:\n",
    "\n",
    "criterion = ['gini', 'entropy']:\n",
    "\n",
    "Chooses the impurity measure for split quality.\n",
    "\n",
    "max_depth = [3, 5, 8, 12, None]:\n",
    "\n",
    "Limits how deep the tree can grow; controls complexity and generalization.\n",
    "\n",
    "min_samples_leaf = [1, 2, 5, 10]:\n",
    "\n",
    "Minimum number of samples required at each leaf node; prevents over-splitting.\n",
    "\n",
    "class_weight = ['balanced', None]:\n",
    "\n",
    "Adjusts for class imbalance in fraud data.\n",
    "\n",
    "Justification:\n",
    "Tuning depth and leaf size prevents overfitting, while testing both gini and entropy ensures the most effective split criterion is selected.\n",
    "Stratified 5-fold CV maintains fraud/legit ratio and ensures consistent evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nundLAip4jOx"
   },
   "source": [
    "2)xxxName of the classifierxxx by xxxxxxxxx(here mention the the name of the classifier and name of the group member worked on it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pwcuDkDh2L5J",
    "outputId": "244a4215-de63-4eda-cc87-ffcaf4d9c0cc"
   },
   "outputs": [],
   "source": [
    "# 4(b) Model building - Logistic Regression\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import numpy as np\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# -----------------------------\n",
    "# Logistic Regression pipeline\n",
    "# -----------------------------\n",
    "pipe_log = Pipeline([\n",
    "    ('pre', preprocessor),  # your transformer (scaling + encoding)\n",
    "    ('clf', LogisticRegression(solver='liblinear', max_iter=500))\n",
    "])\n",
    "\n",
    "param_grid_log = {\n",
    "    'clf__penalty': ['l1', 'l2'],            # L1 = Lasso, L2 = Ridge\n",
    "    'clf__C': [0.01, 0.1, 1, 10],            # inverse regularization strength\n",
    "    'clf__class_weight': [None, 'balanced']  # handle imbalance\n",
    "}\n",
    "\n",
    "grid_log = GridSearchCV(pipe_log, param_grid_log, scoring='roc_auc', cv=cv, n_jobs=-1, verbose=2)\n",
    "grid_log.fit(X_train_80, y_train_80)\n",
    "\n",
    "print(\"LogReg best params:\", grid_log.best_params_)\n",
    "print(\"LogReg best CV ROC AUC:\", grid_log.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "84Fxp5L03qjY"
   },
   "source": [
    "Logistic Regression is a linear model that can easily overfit or underfit depending on the strength of regularization.\n",
    "\n",
    "Hyperparameters tuned:\n",
    "\n",
    "penalty = 'l1' or 'l2':\n",
    "\n",
    "'l1' (Lasso) performs feature selection by shrinking some coefficients to zero.\n",
    "\n",
    "'l2' (Ridge) prevents overfitting by penalizing large coefficients.\n",
    "\n",
    "C = [0.01, 0.1, 1, 10]:\n",
    "\n",
    "Controls inverse regularization strength (smaller = stronger regularization).\n",
    "\n",
    "class_weight = ['balanced', None]:\n",
    "\n",
    "Balances the fraud vs non-fraud classes based on class frequencies.\n",
    "\n",
    "Justification:\n",
    "Tuning these ensures the model has the optimal bias‚Äìvariance balance, avoids overfitting, and correctly handles the class imbalance.\n",
    "Cross-validation (5-fold Stratified) guarantees the results are robust and not dependent on a single split."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XAPg1rPN4vo4"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XjIxjUbT4zqv"
   },
   "source": [
    "3)xxxName of the classifierxxx by xxxxxxxxx(here mention the the name of the classifier and name of the group member worked on it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s0buLT_Q45SW",
    "outputId": "abe5e858-4d2f-4087-d112-bcbe920f1087"
   },
   "outputs": [],
   "source": [
    "# 4(b) Model building - Random Forest\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import numpy as np\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# -----------------------------\n",
    "# Random Forest pipeline (recommended)\n",
    "# -----------------------------\n",
    "pipe_rf = Pipeline([\n",
    "    ('pre', preprocessor),\n",
    "    ('clf', RandomForestClassifier(random_state=42, n_jobs=-1))\n",
    "])\n",
    "\n",
    "param_grid_rf = {\n",
    "    'clf__n_estimators': [100, 200],          # number of trees\n",
    "    'clf__max_depth': [5, 8, 12, None],\n",
    "    'clf__min_samples_leaf': [1, 2, 5],\n",
    "    'clf__class_weight': [None, 'balanced']\n",
    "}\n",
    "\n",
    "grid_rf = GridSearchCV(pipe_rf, param_grid_rf, scoring='roc_auc', cv=cv, n_jobs=-1, verbose=2)\n",
    "grid_rf.fit(X_train_80, y_train_80)\n",
    "\n",
    "print(\"RandomForest best params:\", grid_rf.best_params_)\n",
    "print(\"RandomForest best CV ROC AUC:\", grid_rf.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uFT8VIkH3w-e"
   },
   "source": [
    "Random Forest is an ensemble of trees; tuning controls its bias‚Äìvariance tradeoff and computational efficiency.\n",
    "\n",
    "Hyperparameters tuned:\n",
    "\n",
    "n_estimators = [100, 200]:\n",
    "\n",
    "Number of trees; more trees improve stability but increase training time.\n",
    "\n",
    "max_depth = [5, 8, 12, None]:\n",
    "\n",
    "Restricts tree growth; prevents overfitting on small datasets.\n",
    "\n",
    "min_samples_leaf = [1, 2, 5]:\n",
    "\n",
    "Minimum leaf size for regularization.\n",
    "\n",
    "class_weight = ['balanced', None]:\n",
    "\n",
    "Adjusts for the under-represented fraud class.\n",
    "\n",
    "Justification:\n",
    "These parameters directly influence model complexity and variance.\n",
    "Tuning ensures optimum generalization while controlling training time.\n",
    "Using cross-validation with ROC-AUC scoring ensures that results are consistent and threshold-independent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Fsc2yMTW22wn",
    "outputId": "b15b2a7d-26b2-4664-9688-dc901ac0d680"
   },
   "outputs": [],
   "source": [
    "# 4(b) Model building - SVM\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "import numpy as np\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "# -----------------------------\n",
    "# SVM pipeline & grid\n",
    "# -----------------------------\n",
    "\n",
    "pipe_svc = Pipeline([\n",
    "    ('pre', preprocessor),\n",
    "    ('clf', SVC(probability=True, class_weight='balanced', random_state=42))\n",
    "])\n",
    "\n",
    "param_grid_svc = {\n",
    "    'clf__kernel': ['linear', 'rbf'],\n",
    "    'clf__C': [0.1, 1, 10],\n",
    "    # for rbf kernel consider gamma scale or custom grid\n",
    "    'clf__gamma': ['scale', 'auto']\n",
    "}\n",
    "\n",
    "grid_svc = GridSearchCV(pipe_svc, param_grid_svc, scoring='roc_auc', cv=cv, n_jobs=-1, verbose=2)\n",
    "grid_svc.fit(X_train_80, y_train_80)\n",
    "\n",
    "print(\"SVM best params:\", grid_svc.best_params_)\n",
    "print(\"SVM best CV ROC AUC:\", grid_svc.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zAaS5qvN3yBo"
   },
   "source": [
    "SVM performance depends strongly on the kernel type and the parameters that define its margin and curvature.\n",
    "\n",
    "Hyperparameters tuned:\n",
    "\n",
    "kernel = ['linear', 'rbf']:\n",
    "\n",
    "Linear handles linearly separable data; RBF captures non-linear boundaries.\n",
    "\n",
    "C = [0.1, 1, 10]:\n",
    "\n",
    "Controls margin softness (regularization). Small C ‚Üí wider margin, fewer misclassifications.\n",
    "\n",
    "gamma = ['scale', 'auto']:\n",
    "\n",
    "Defines curvature of the decision boundary for RBF kernel.\n",
    "\n",
    "class_weight = 'balanced':\n",
    "\n",
    "Ensures fraud cases are not ignored due to imbalance.\n",
    "\n",
    "Justification:\n",
    "Tuning C and gamma optimizes margin width and boundary flexibility, preventing both over- and underfitting.\n",
    "Stratified 5-fold CV guarantees robust and fair comparison across kernel and parameter combinations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4pDJKlKT3I-W",
    "outputId": "4a4626e5-4303-44ca-c574-0f3495016fce"
   },
   "outputs": [],
   "source": [
    "# 4(b) Model building - KNN\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# -----------------------------\n",
    "# KNN Pipeline\n",
    "# -----------------------------\n",
    "pipe_knn = Pipeline([\n",
    "    ('pre', preprocessor),  # scaling + encoding\n",
    "    ('clf', KNeighborsClassifier())\n",
    "])\n",
    "\n",
    "# --- Define parameter grid ---\n",
    "param_grid_knn = {\n",
    "    'clf__n_neighbors': [3, 5, 7, 9, 11],\n",
    "    'clf__weights': ['uniform', 'distance'],   # weight by distance or not\n",
    "    'clf__metric': ['euclidean', 'manhattan']  # distance function\n",
    "}\n",
    "\n",
    "# --- Perform Grid Search ---\n",
    "grid_knn = GridSearchCV(pipe_knn, param_grid_knn, scoring='roc_auc', cv=cv, n_jobs=-1, verbose=2)\n",
    "grid_knn.fit(X_train_80, y_train_80)\n",
    "\n",
    "print(\"KNN best params:\", grid_knn.best_params_)\n",
    "print(\"KNN best CV ROC AUC:\", grid_knn.best_score_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uJczMYAQ36ED"
   },
   "source": [
    "KNN is highly sensitive to the number of neighbors and the distance metric; improper settings can cause under- or overfitting.\n",
    "\n",
    "Hyperparameters tuned:\n",
    "\n",
    "* n_neighbors = [3, 5, 7, 9, 11]:\n",
    "\n",
    "Controls model smoothness; low values overfit, high values underfit.\n",
    "\n",
    "* weights = ['uniform', 'distance']:\n",
    "\n",
    "Distance weighting reduces the influence of farther neighbors.\n",
    "\n",
    "* metric = ['euclidean', 'manhattan']:\n",
    "\n",
    "Different distance functions can yield better performance depending on data structure.\n",
    "\n",
    "Justification:\n",
    "Tuning these ensures an optimal neighborhood size and weighting for accurate fraud detection.\n",
    "Cross-validation ensures that the choice of k generalizes across the dataset and is not dependent on random splits.\n",
    "Standardization in preprocessing ensures all features contribute equally to distance computation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XJ3l6AcB45wP"
   },
   "source": [
    "5.\tPerformance Evaluation [2M]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "B6rbcBU04-UT",
    "outputId": "d17d8db8-64ba-4fbf-e753-b2302019a145"
   },
   "outputs": [],
   "source": [
    "# 5(a) Compare Logistic Regression vs Decision Tree\n",
    "\n",
    "import warnings\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score, roc_auc_score,\n",
    "    confusion_matrix, classification_report\n",
    ")\n",
    "import pandas as pd\n",
    "\n",
    "# Suppress the undefined metric warnings (for clean output)\n",
    "from sklearn.exceptions import UndefinedMetricWarning\n",
    "warnings.filterwarnings(\"ignore\", category=UndefinedMetricWarning)\n",
    "\n",
    "# Retrieve the tuned/best models from GridSearchCV\n",
    "best_log = grid_log.best_estimator_\n",
    "best_dt  = grid_dt.best_estimator_\n",
    "\n",
    "# --- Predict on test set ---\n",
    "y_pred_log = best_log.predict(X_test_20)\n",
    "y_pred_dt  = best_dt.predict(X_test_20)\n",
    "\n",
    "# --- Probabilities for ROC-AUC ---\n",
    "y_proba_log = best_log.predict_proba(X_test_20)[:, 1]\n",
    "y_proba_dt  = best_dt.predict_proba(X_test_20)[:, 1]\n",
    "\n",
    "# --- Function to evaluate models ---\n",
    "def evaluate_model(name, y_true, y_pred, y_proba):\n",
    "    acc  = accuracy_score(y_true, y_pred)\n",
    "    prec = precision_score(y_true, y_pred, zero_division=0)\n",
    "    rec  = recall_score(y_true, y_pred, zero_division=0)\n",
    "    f1   = f1_score(y_true, y_pred, zero_division=0)\n",
    "    roc  = roc_auc_score(y_true, y_proba)\n",
    "\n",
    "    print(f\"\\n=== {name} Performance ===\")\n",
    "    print(classification_report(y_true, y_pred, digits=4, zero_division=0))\n",
    "    print(\"Confusion Matrix:\\n\", confusion_matrix(y_true, y_pred))\n",
    "\n",
    "    return [acc, prec, rec, f1, roc]\n",
    "\n",
    "# --- Evaluate both models ---\n",
    "metrics = pd.DataFrame(\n",
    "    [\n",
    "        evaluate_model(\"Logistic Regression\", y_test_20, y_pred_log, y_proba_log),\n",
    "        evaluate_model(\"Decision Tree\", y_test_20, y_pred_dt, y_proba_dt)\n",
    "    ],\n",
    "    columns=[\"Accuracy\", \"Precision\", \"Recall\", \"F1-Score\", \"ROC-AUC\"],\n",
    "    index=[\"Logistic Regression\", \"Decision Tree\"]\n",
    ")\n",
    "\n",
    "# --- Display comparison table ---\n",
    "print(\"\\n=== Model Comparison Table ===\")\n",
    "display(metrics)\n",
    "\n",
    "# --- Optional: bar plot comparison ---\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "metrics.plot(kind='bar', figsize=(10,6))\n",
    "plt.title(\"Model Performance Comparison: Logistic Regression vs Decision Tree\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.xticks(rotation=0)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dw33B-qO5LYP"
   },
   "source": [
    "The bar chart compares Accuracy, Precision, Recall, F1-Score, and ROC-AUC for Logistic Regression and Decision Tree side by side.\n",
    "\n",
    "Logistic Regression shows:\n",
    "\n",
    "Higher Accuracy ‚Äî it correctly classifies most transactions.\n",
    "\n",
    "Higher ROC-AUC ‚Äî better overall discrimination between fraud and legitimate cases.\n",
    "\n",
    "Decision Tree shows:\n",
    "\n",
    "Higher Recall ‚Äî it identifies a larger proportion of fraudulent transactions.\n",
    "\n",
    "Slightly lower Precision ‚Äî meaning it also flags more false positives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Aki8gNwj4_n1"
   },
   "source": [
    "Remarks(If any)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VsLfvTKA5r9w"
   },
   "source": [
    "The Decision Tree trades off precision for recall, which is common in fraud detection systems designed to catch more frauds (even if that means investigating more legitimate transactions).\n",
    "\n",
    "The Logistic Regression is more conservative ‚Äî fewer false alarms but slightly more missed frauds ‚Äî ideal for stable, production-ready models.\n",
    "\n",
    "Both models perform comparably on ROC-AUC, confirming that both are reliable classifiers but optimized for different objectives.\n",
    "\n",
    "üîπ Justification for Business Context\n",
    "\n",
    "In fraud detection, Recall is more important than Accuracy ‚Äî it‚Äôs better to flag a few extra normal transactions than miss an actual fraud.\n",
    "\n",
    "Hence, while Logistic Regression offers better balance, Decision Tree may be preferred in high-risk, fraud-sensitive environments due to its higher recall.\n",
    "\n",
    "On the other hand, if we want to minimize false alerts and ensure user trust, Logistic Regression would be the better operational model."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
